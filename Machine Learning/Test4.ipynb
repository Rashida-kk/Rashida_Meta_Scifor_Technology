{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Algorithm questions\n"
      ],
      "metadata": {
        "id": "2fJik0HrqlNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.How does regularization (L1 and L2) help in preventing overfitting?\n",
        "\n",
        "Ans: Regularization helps prevent overfitting by adding a penalty to the model's complexity, discouraging it from relying too much on specific features.The two common types of regularization are L1 regularization (Lasso) and L2 regularization (Ridge).\n",
        "\n",
        "\n",
        "\n",
        "*   L1 simplifies the model by eliminating irrelevant features.\n",
        "*   L2 keeps all features but reduces their influence, leading to a smoother, generalized model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DdnrbBkarENX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Why is feature scaling important in gradient descent?\n",
        "\n",
        "Ans: Feature scaling ensures that all features contribute equally in gradient descent by putting them on the same scale. This prevents larger features from dominating, speeds up convergence, and avoids calculation issues.\n",
        "\n",
        "Common Scaling Methods:\n",
        "\n",
        "1. Min-Max Scaling: Makes features range from 0 to 1.\n",
        "\n",
        "2. Standardization: Adjusts features to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "bAw8U0cCuBE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem Solving\n"
      ],
      "metadata": {
        "id": "PmdtKp-9wxu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Given a dataset with missing values, how would you handle them before training an ML model?\n",
        "\n",
        "Ans: Handling missing values before training a machine learning model involves the following :\n",
        "\n",
        "        # Check for missing values\n",
        "        print(data.isnull().sum())\n",
        "\n",
        "        # Remove rows with missing values\n",
        "        data_cleaned_raws = data.dropna()\n",
        "        \n",
        "        # Remove columns with too many missing values\n",
        "        data_cleaned_cols = data.dropna(axis=1)\n",
        "\n",
        "        # Fill missing numerical values with the mean\n",
        "        data['numerical_column'] = data['numerical_column'].fillna(data['numerical_column'].mean())\n",
        "\n",
        "        # Fill missing categorical values with the mode\n",
        "        data['categorical_column'] = data['categorical_column'].fillna(data['categorical_column'].mode()[0])\n",
        "\n",
        "        # Forward fill for time-series or sequential data\n",
        "        data['time_series_column'] = data['time_series_column'].fillna(method='ffill')\n",
        "\n",
        "        # Backward fill for time-series or sequential data\n",
        "        data['time_series_column'] = data['time_series_column'].fillna(method='bfill')\n",
        "\n",
        "        # Verify no missing values remain\n",
        "        print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "UdzVseJ0w3sP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Design a pipeline for building a classification model. Include steps for data preprocessing.\n",
        "\n",
        "Ans: 1. Data Preprocessing\n",
        "\n",
        "a. Data Loading\n",
        "Load the dataset using pandas or another library.\n",
        "\n",
        "b. Exploratory Data Analysis (EDA)\n",
        "Check for missing values, data distribution, outliers, and class balance.\n",
        "Use data.info(), data.describe(), and visualization libraries like matplotlib or seaborn.\n",
        "\n",
        "c. Handle Missing Values\n",
        "Impute missing values using mean, median, mode, or advanced techniques.\n",
        "\n",
        "d. Encode Categorical Features\n",
        "Convert categorical variables to numerical ones using:\n",
        "Label Encoding: For ordinal categories.\n",
        "One-Hot Encoding: For nominal categories.\n",
        "\n",
        "e. Feature Scaling\n",
        "Standardize or normalize numerical features for gradient-based algorithms (e.g., Logistic Regression, SVM).\n",
        "\n",
        "f. Feature Selection\n",
        "Use correlation analysis, feature importance, or PCA to select the most relevant features.\n",
        "\n",
        "2. Model Building\n",
        "\n",
        "a. Train-Test Split\n",
        "Split the data into training and test sets using train_test_split() from sklearn.\n",
        "\n",
        "b. Model Selection\n",
        "Choose a classification algorithm (e.g., Logistic Regression, Decision Tree, Random Forest, SVM, or Neural Network).\n",
        "\n",
        "c. Model Training\n",
        "Train the model using the training data.\n",
        "\n",
        "d. Hyperparameter Tuning\n",
        "Use Grid Search or Random Search to optimize model parameters.\n",
        "\n",
        "3. Model Evaluation\n",
        "\n",
        "a. Evaluate on Test Data\n",
        "Use metrics like accuracy, precision, recall, F1-score, ROC-AUC, and confusion matrix.\n",
        "\n",
        "b. Cross-Validation\n",
        "Perform k-fold cross-validation to assess model stability and generalization.\n"
      ],
      "metadata": {
        "id": "vMNxC9aJ0Su3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eonQw-PRqR3k"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Step 2: Load data\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Step 3: Define features (X) and target (y)\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Step 4: Data Preprocessing\n",
        "# Identify numerical and categorical features\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Numerical feature processing (impute missing values and scale)\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
        "    ('scaler', StandardScaler())])  # Standardize numerical data\n",
        "\n",
        "# Categorical feature processing (impute missing values and encode)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])  # One-Hot Encoding for categorical features\n",
        "\n",
        "# Combine the numeric and categorical transformations\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Step 5: Create full pipeline (Preprocessing + Model)\n",
        "clf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing steps\n",
        "    ('classifier', RandomForestClassifier())])  # Classification model\n",
        "\n",
        "# Step 6: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "clf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on test set\n",
        "y_pred = clf_pipeline.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Coding"
      ],
      "metadata": {
        "id": "Wlo06dzX4uQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Write a Python script to implement a decision tree classifier using Scikit-learn.\n"
      ],
      "metadata": {
        "id": "ERelRh0f3JHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(dataset.csv)\n",
        "\n",
        "X = data.drop('target', axis=1)  # Features\n",
        "y = data['target']  # Target\n",
        "\n",
        "# Step 3: Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Create a Decision Tree model\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Step 5: Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 8: Visualizing the Decision Tree\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(model, filled=True, feature_names=data.feature_names, class_names=data.target_names, rounded=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dx3IRFmf3n4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Given a dataset, write code to split the data into training and testing sets using an 80-20 split.\n"
      ],
      "metadata": {
        "id": "73in5uxa4x-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)    #features\n",
        "y = data['target']          #target\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "FlqQ4Gdu5Sk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Case Study\n"
      ],
      "metadata": {
        "id": "eYP_QDzU5wDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A company wants to predict employee attrition. What kind of ML problem is this? Which algorithms would you choose and why?\n",
        "\n",
        "Ans: Predicting employee attrition is a supervised classification problem, as the goal is to predict a categorical outcome (whether an employee will leave or stay) based on historical data. The target variable (attrition) is usually binary: 1 (employee leaves) or 0 (employee stays).\n",
        "\n",
        "Algorithms to Use:\n",
        "\n",
        "1. Logistic Regression:\n",
        "\n",
        "Simple and effective for predicting binary outcomes.\n",
        "\n",
        "Easy to interpret the results.\n",
        "\n",
        "2. Decision Tree:\n",
        "\n",
        "Good for understanding how features (like age, job satisfaction) affect attrition.\n",
        "\n",
        "Can be visualized for better understanding.\n",
        "\n",
        "3. Random Forest:\n",
        "\n",
        "Combines multiple decision trees to improve accuracy.\n",
        "\n",
        "Handles complex datasets well and reduces overfitting\n",
        "\n",
        "Why These Algorithms?:\n",
        "\n",
        "Logistic Regression and Decision Trees are simple and interpretable.\n",
        "\n",
        "Random Forest improve performance on more complex datasets.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Clean the data (handle missing values, encode categories).\n",
        "\n",
        "Split the data into training and testing sets.\n",
        "\n",
        "Train and evaluate models.\n",
        "\n",
        "Use performance metrics like accuracy and F1-score to choose the best model."
      ],
      "metadata": {
        "id": "GDOfk7Uq53Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unFqEEjK7g6p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}