{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB3v0oq1VLTe",
        "outputId": "51803102-de8a-4353-8164-1393abe1a2dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "#Installation\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "1PiajjnVgywg",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Downloads the Punkt Tokenizer Models\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1umcMWCij9He",
        "outputId": "530fc965-d403-4654-a66f-e54956f336d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command nltk.download('punkt') downloads the Punkt Tokenizer Models, a pre-trained model in NLTK that is essential for text tokenization. The Punkt tokenizer is a sentence and word tokenization model specifically trained to split text into individual sentences and words based on punctuation and other linguistic clues."
      ],
      "metadata": {
        "id": "rv3QL35gk29n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the punkt_tab tokenizer model\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thciB62CP6KY",
        "outputId": "1b9ed72d-e0b4-4a85-b767-2a1b2c867d6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.tokenization\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Success is not final, failure is not fatal. It is the courage to continue that counts.\"\n",
        "\n",
        "# word tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", words)\n",
        "\n",
        "# sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKtdGo34f_QE",
        "outputId": "0a68d61b-e015-4802-d7a9-cb62c0f36788"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Success', 'is', 'not', 'final', ',', 'failure', 'is', 'not', 'fatal', '.', 'It', 'is', 'the', 'courage', 'to', 'continue', 'that', 'counts', '.']\n",
            "Sentence Tokenization: ['Success is not final, failure is not fatal.', 'It is the courage to continue that counts.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'stopwords' dataset\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQAcgYuBQWRH",
        "outputId": "e26ccd8b-e57e-4bb2-b36e-c1e5c87dac16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.stop words removal\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "words = word_tokenize(\"This is a simple sentence for testing\")\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "id": "dctLUsRjjhwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b13104b-b6ce-49ac-e72d-532519756499"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['simple', 'sentence', 'testing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming and Lemmatization\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "#stemming\n",
        "ps = PorterStemmer()\n",
        "print(\"stemmed words:\", [ps.stem(word) for word in filtered_words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKzbdrZXz6UD",
        "outputId": "641b19a3-a1d9-4d71-9385-deec9200813f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stemmed words: ['simpl', 'sentenc', 'test']\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#Lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"Lemmatized word:\", [lemmatizer.lemmatize(word) for word in filtered_words]) # Use lemmatizer.lemmatize(word) instead of lemmatizer(word)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6QxUyZi4g3t",
        "outputId": "8fc43d4a-f2c4-408d-cc8a-f8ffcb071783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized word: ['simple', 'sentence', 'testing']\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#part of speech (pos) tagging\n",
        "import nltk\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "# Corrected the function name from word_tokanize to word_tokenize\n",
        "pos_tags = nltk.pos_tag(nltk.word_tokenize(\"NLTK is amazing for NLP tasks!\"))\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLXVtH4BCqmH",
        "outputId": "edc9ea70-cfff-4cf0-e43c-266112c6df37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('NLTK', 'NNP'), ('is', 'VBZ'), ('amazing', 'VBG'), ('for', 'IN'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('!', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Named Entity Recognition (NER)\n",
        "import nltk\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "\n",
        "# Import the ne_chunk function from nltk.chunk\n",
        "from nltk.chunk import ne_chunk\n",
        "\n",
        "# Now use ne_chunk to extract entities\n",
        "entities = ne_chunk(pos_tags)\n",
        "print(\"Named Entities:\", entities)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klAZdTO4Eriz",
        "outputId": "81994fed-10c3-4273-f3eb-ef09f60903e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: (S\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  is/VBZ\n",
            "  amazing/VBG\n",
            "  for/IN\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  tasks/NNS\n",
            "  !/.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis (Using Pre-trained Models)\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment = sia.polarity_scores(\"NLTK is incredibly helpful for NLP tasks!\")\n",
        "print(\"Sentiment:\", sentiment)\n"
      ],
      "metadata": {
        "id": "yLj_z5NIEr8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e06ce90-db60-4f82-8be9-f09a2bd1d8b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: {'neg': 0.0, 'neu': 0.639, 'pos': 0.361, 'compound': 0.5244}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P5jPmxRRQbto"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}